{
 "cells": [
  {
   "cell_type": "code",
   "id": "f73df763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T21:22:53.467620Z",
     "start_time": "2025-07-01T21:22:51.797745Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import madmom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pyrubberband as pyrb\n",
    "import soundfile\n",
    "import soundfile as sf\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Audio\n",
    "from madmom.features.downbeats import DBNDownBeatTrackingProcessor, RNNDownBeatProcessor\n",
    "from pydub import AudioSegment\n",
    "from scipy.io.wavfile import write\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MutableSequence' from 'collections' (C:\\Users\\sycom\\miniconda3\\envs\\AI-DJ-Mix-Generator\\Lib\\collections\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlibrosa\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlibrosa\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdisplay\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmadmom\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\AI-DJ-Mix-Generator\\Lib\\site-packages\\madmom\\__init__.py:24\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpkg_resources\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# import all packages\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m audio, evaluation, features, io, ml, models, processors, utils\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# define a version variable\u001B[39;00m\n\u001B[32m     27\u001B[39m __version__ = pkg_resources.get_distribution(\u001B[33m\"\u001B[39m\u001B[33mmadmom\u001B[39m\u001B[33m\"\u001B[39m).version\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\AI-DJ-Mix-Generator\\Lib\\site-packages\\madmom\\audio\\__init__.py:27\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m__future__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m absolute_import, division, print_function\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# import the submodules\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m comb_filters, filters, signal, spectrogram, stft\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# import classes used often\u001B[39;00m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mchroma\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DeepChromaProcessor\n",
      "\u001B[36mFile \u001B[39m\u001B[32mmadmom\\\\audio\\\\comb_filters.pyx:15\u001B[39m, in \u001B[36minit madmom.audio.comb_filters\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\AI-DJ-Mix-Generator\\Lib\\site-packages\\madmom\\processors.py:23\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcollections\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MutableSequence\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m integer_types\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'MutableSequence' from 'collections' (C:\\Users\\sycom\\miniconda3\\envs\\AI-DJ-Mix-Generator\\Lib\\collections\\__init__.py)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "aabecb01",
   "metadata": {},
   "source": [
    "def detect_downbeats(audio_file, sr=44100, fps=100):\n",
    "    # Load the audio signal without resampling\n",
    "    signal = madmom.audio.signal.Signal(audio_file, sample_rate=sr, num_channels=1)\n",
    "    \n",
    "    # Get the beat and downbeat activations\n",
    "    act = madmom.features.RNNDownBeatProcessor()(signal)\n",
    "    \n",
    "    # Create a downbeat processor\n",
    "    proc = madmom.features.DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=fps)\n",
    "    \n",
    "    # Get the beats and downbeats\n",
    "    beats_and_downbeats = proc(act)\n",
    "    \n",
    "    # Filter downbeats (those with beat position 1)\n",
    "    downbeats = [beat for beat in beats_and_downbeats if beat[1] == 1]\n",
    "\n",
    "    return np.array(downbeats)\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "def estimate_tempo_from_downbeats(audio_file, downbeats):\n",
    "    \n",
    "    # Calculate the time difference between consecutive downbeats\n",
    "    downbeat_differences = np.diff(downbeats[:, 0])\n",
    "\n",
    "    # Calculate the time difference between consecutive downbeats\n",
    "    downbeat_differences = np.around(np.diff(downbeats[:, 0]), decimals=6)\n",
    "\n",
    "    # Get the mode of the differences\n",
    "    mod_diff = mode(downbeat_differences).mode\n",
    "\n",
    "    # Calculate the tempo: 60 seconds divided by the average difference\n",
    "    # Since downbeat_differences are in seconds, this gives beats per minute\n",
    "    tempo = 4 * (60 / mod_diff)\n",
    "    \n",
    "    tempo = round(tempo)\n",
    "\n",
    "    return tempo, mod_diff, downbeat_differences\n",
    "\n",
    "def estimate_beat(audio_file, sr=44100):\n",
    "    try:\n",
    "        # Load the audio signal with the specified sample rate\n",
    "        signal = madmom.audio.signal.Signal(audio_file, sample_rate=sr, num_channels=1)\n",
    "        \n",
    "        # Consider only 4/4 time signature\n",
    "        proc = DBNDownBeatTrackingProcessor(beats_per_bar=[4], fps=100)\n",
    "        act = RNNDownBeatProcessor()(signal)\n",
    "        proc_res = proc(act)\n",
    "        \n",
    "        # Extract the beat timestamps\n",
    "        beat_timestamps = proc_res[:, 0]\n",
    "        \n",
    "        return beat_timestamps\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return np.array([])  # Return an empty array for better consistency"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c486d0c1",
   "metadata": {},
   "source": [
    "class SimpleTrack:\n",
    "    def __init__(self, name, wav_file):\n",
    "        self.name = name\n",
    "        self.wav_file = wav_file\n",
    "        self.audio, self.sr = librosa.load(wav_file, sr=None)\n",
    "        self.downbeats = None\n",
    "        self.beats = None\n",
    "        self.tempo = None\n",
    "        self.cueyes = None\n",
    "        self.cueno = None\n",
    "        self.time_signature = None\n",
    "        self.x_tensor = None\n",
    "        self.cue_points = None\n",
    "        self.cue_points_indeces = None \n",
    "        \n",
    "    def prepare_features_for_prediction(self):\n",
    "        X_new = [self.convert_to_feature_vector(f) for f in self.features]\n",
    "        X_new = scaler.transform(X_new)  # Normalize\n",
    "        self.x_tensor = torch.FloatTensor(X_new).unsqueeze(0)\n",
    "        \n",
    "    \n",
    "    def detect_downbeats(self):\n",
    "        self.downbeats = detect_downbeats(self.wav_file)\n",
    "\n",
    "    def estimate_tempo_from_downbeats(self):\n",
    "        self.tempo, _, self.downbeat_differences = estimate_tempo_from_downbeats(self.wav_file, self.downbeats)\n",
    "        \n",
    "    def estimate_beat(self):\n",
    "        self.beats = estimate_beat(self.wav_file)\n",
    "        \n",
    "    # 1. Convert the extracted features into flat feature vectors\n",
    "    def convert_to_feature_vector(self, beat_features):\n",
    "        \"\"\"Convert beat features into a flattened feature vector.\"\"\"\n",
    "        flat_features = list(beat_features['MFCC']) + \\\n",
    "                        list(beat_features['Spectral Contrast']) + \\\n",
    "                        [beat_features['Spectral Centroid']] + \\\n",
    "                        [beat_features['Spectral Rolloff']] + \\\n",
    "                        [beat_features['Spectral Flux']] + \\\n",
    "                        [beat_features['RMS']]\n",
    "        return flat_features"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c8521c7",
   "metadata": {},
   "source": [
    "# specify your path for the training set \n",
    "\n",
    "path = \"kasi\"\n",
    "\n",
    "raw_wav_files = [os.path.join(path, file_name) for file_name in os.listdir(path) if file_name.endswith('.wav')]\n",
    "\n",
    "raw_tracks = {}\n",
    "counter = {}\n",
    "\n",
    "for wav_file in raw_wav_files:\n",
    "    track_name = (os.path.basename(wav_file))\n",
    "    if track_name in counter:\n",
    "        counter[track_name] += 1\n",
    "        track_name = f\"{track_name}{counter[track_name]}\"\n",
    "    else:\n",
    "        counter[track_name] = 1\n",
    "    track = SimpleTrack(track_name, wav_file)\n",
    "    raw_tracks[track_name] = track\n",
    "\n",
    "tracks = raw_tracks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e45e3e8a",
   "metadata": {},
   "source": [
    "tracks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73ac9514",
   "metadata": {},
   "source": [
    "def preprocess_simple_track(track):\n",
    "    \n",
    "    # Detect downbeats\n",
    "    track.detect_downbeats()\n",
    "    \n",
    "    # Estimate tempo from downbeats\n",
    "    track.estimate_tempo_from_downbeats()\n",
    "    \n",
    "    print(f\"Estimated Tempo for {track.name}: {track.tempo:.2f} BPM\")\n",
    "    track.estimate_beat()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2a769b6",
   "metadata": {},
   "source": [
    "# Calling the preprocess function on each track in the tracks dictionary\n",
    "for track_name, track_obj in tracks.items():\n",
    "    print(f\"Preprocessing : {track_obj.name}\")\n",
    "    preprocess_simple_track(track_obj)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3da07ac3",
   "metadata": {},
   "source": [
    "### REKORDBOX "
   ]
  },
  {
   "cell_type": "code",
   "id": "c833966c",
   "metadata": {},
   "source": [
    "aligned_track_names = {\n",
    "    'Amniote Editions - JD Typo - Txtural': 'Amniote',\n",
    "    'BEADS - SET THE TEMPO': 'BEADS',\n",
    "    'DJ SWISHERMAN - Big Boy [RYS002]': 'DJ',\n",
    "    'Gene Richards Jr - The Feeder (Chlär & Chontane Remix)': 'Gene',\n",
    "    'JASØN - JASØN - Fura (Edvvin Remix)': 'JASØN',\n",
    "    'kvlr - The Navigator [MASTER]': 'kvlr',\n",
    "    'MiniTribe - Pharpheonix': 'MiniTribe',\n",
    "    'PETERBLUE - Latin Club (Reformist Remix)': 'PETERBLUE',\n",
    "    'Seigg - Needless': 'Seigg',\n",
    "    \"SJUSH - Pacman's Deathwish\": 'SJUSH',\n",
    "    'Shannen Blessing - Take Me Out': 'Shannen',\n",
    "    'Stef de Haan - In 4 The Kill (Stef de Haan Remix)': 'Stef',\n",
    "    'Bad Boombox - RAW Summer Hits 4 • Original Tracks - 20 Gushy (Ft. MC Yung Lil)': 'Bad',\n",
    "    'CAIVA - RAW Summer Hits 4 • Original Tracks - 17 The Love Inside Me': 'CAIVA',\n",
    "    'Dallaniel - RAW Summer Hits 4 • Original Tracks - 14 Somebody Else': 'Dallaniel',\n",
    "    'Dylan Fogarty - RAW Summer Hits 4 • Original Tracks - 02 Da Funky Track': 'Dylan',\n",
    "    'Mac Declos & LAZE - RAW Summer Hits 4 • Original Tracks - 19 Wanted Prisma': 'Mac',\n",
    "    'RUIZ OSC1 - RAW Summer Hits 4 • Original Tracks - 03 Snare 3000': 'RUIZ',\n",
    "    'The Chronics - RAW Summer Hits 4 • Original Tracks - 13 Luz': 'The'}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4fac6d93",
   "metadata": {},
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_hot_cues_for_aligned_tracks(xml_file_path, aligned_track_names):\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    hot_cue_points = {}\n",
    "    \n",
    "    # Iterate over all tracks in the XML\n",
    "    for track in root.findall(\".//TRACK\"):\n",
    "        track_name = track.get(\"Name\")\n",
    "        if track_name not in aligned_track_names:\n",
    "            continue  # skip tracks not in the alignment\n",
    "        \n",
    "        cues = []\n",
    "        for cue in track.findall(\".//POSITION_MARK\"):\n",
    "            # Convert the start time from milliseconds to seconds\n",
    "            start_time = float(cue.get(\"Start\"))\n",
    "            cues.append(start_time)\n",
    "            \n",
    "        hot_cue_points[track_name] = cues\n",
    "\n",
    "    return hot_cue_points\n",
    "\n",
    "# Usage:\n",
    "rekordbox_xml_path = \"rekordbox_meta_data.xml\"\n",
    "cue_points = extract_hot_cues_for_aligned_tracks(rekordbox_xml_path, aligned_track_names)\n",
    "print(cue_points)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "121e24f5",
   "metadata": {},
   "source": [
    "for track_name, track_obj in tracks.items():\n",
    "    track_name_without_extension = track_name.replace('.wav', '')\n",
    "    if track_name_without_extension in cue_points:\n",
    "        track_obj.cueyes = cue_points[track_name_without_extension]\n",
    "    else:\n",
    "        print(f\"No hot cues found for track name: {track_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a15dae9a",
   "metadata": {},
   "source": [
    "for track_name, track_obj in tracks.items():\n",
    "    print(f\"Track Name: {track_name}\")\n",
    "    print(f\"Hot Cue Points: {track_obj.cueyes}\")\n",
    "    print(\"=====================================\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c70a60e",
   "metadata": {},
   "source": [
    "def plot_audio_segment_with_beats_and_cues(audio, sr, beats, cue_points, start_time, duration):\n",
    "    # Calculate start and end samples\n",
    "    start_sample = int(start_time * sr)\n",
    "    end_sample = start_sample + int(duration * sr)\n",
    "\n",
    "    # Create time array\n",
    "    time = np.arange(start_sample, end_sample) / sr\n",
    "\n",
    "    # Identify beats and cue points in the segment\n",
    "    segment_beats = [beat for beat in beats if start_time <= beat < start_time + duration]\n",
    "\n",
    "    segment_cues = [float(cue) for cue in cue_points if start_time <= float(cue) < start_time + duration]\n",
    "\n",
    "\n",
    "    # Plot audio segment\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(time, audio[start_sample:end_sample])\n",
    "    plt.vlines(segment_beats, ymin=np.min(audio), ymax=np.max(audio), colors='r', label='Beats')\n",
    "    plt.vlines(segment_cues, ymin=np.min(audio), ymax=np.max(audio), colors='b', linestyle='dashed', label='Cue Points')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Audio Segment with Beats and Cue Points')\n",
    "    plt.legend()\n",
    "    plt.xlim([start_time, start_time + duration])\n",
    "    plt.show()\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "baf00b98",
   "metadata": {},
   "source": [
    "def plot_audio_segment_around_first_and_last_cue(audio, sr, beats, cue_points):\n",
    "    # Extract only the first and last cue points\n",
    "    first_cue = cue_points[1]\n",
    "    last_cue = cue_points[-2]\n",
    "    selected_cue_points = [first_cue, last_cue]\n",
    "    \n",
    "    for cue in selected_cue_points:\n",
    "        cue = float(cue)  # Convert cue point from string to float\n",
    "        \n",
    "        # Define start time and duration for the segment centered around the cue point\n",
    "        start_time = max(cue - 10, 0)  # Ensure the start time is not negative\n",
    "        duration = 20\n",
    "        \n",
    "        # Calculate end time, ensuring it doesn't exceed the length of the audio\n",
    "        end_time = min(cue + 10, len(audio) / sr)\n",
    "        \n",
    "        # Adjust duration if end_time is at the end of the audio\n",
    "        if end_time == len(audio) / sr:\n",
    "            start_time = end_time - 20\n",
    "        \n",
    "        # If cue point is near the beginning or end, adjust start_time to ensure cue is within plotting window\n",
    "        if cue - start_time < 5:  # If cue is within first 5 seconds of window\n",
    "            start_time = cue - 5\n",
    "        elif end_time - cue < 5:  # If cue is within last 5 seconds of window\n",
    "            start_time = cue - 15\n",
    "\n",
    "        # Ensure start_time and duration values are valid\n",
    "        start_time = max(0, start_time)\n",
    "        duration = min(duration, len(audio) / sr - start_time)\n",
    "\n",
    "        # Plot the segment\n",
    "        plot_audio_segment_with_beats_and_cues(audio, sr, beats, np.array(selected_cue_points), start_time, duration)\n",
    "\n",
    "for track_name, track in tracks.items():\n",
    "    print(f\"Plotting for track: {track_name}\")\n",
    "    audio = track.audio\n",
    "    sr = track.sr\n",
    "    beats = track.beats\n",
    "    cue_points = track.cueyes  # Use the cue_points attribute of the track\n",
    "    plot_audio_segment_around_first_and_last_cue(audio, sr, beats, cue_points)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "234ae868",
   "metadata": {},
   "source": [
    "def label_cues(beats, cueyes):\n",
    "    \"\"\"Label the cues and their neighbors.\"\"\"\n",
    "    # Start with an array of -1 (unlabeled)\n",
    "    labels = np.ones(len(beats)) * -1\n",
    "\n",
    "    # Convert cueyes to indices\n",
    "    cueyes_indices = []\n",
    "    for cue in cueyes:\n",
    "        cue_index = np.argmin(np.abs(np.array(beats) - cue))\n",
    "        cueyes_indices.append(cue_index)\n",
    "        labels[cue_index] = 1\n",
    "\n",
    "    # Label the neighboring beats within the radius\n",
    "    cueno_indices = []\n",
    "    for idx in cueyes_indices:\n",
    "        for offset in range(-3, 4):  # Include the range from -3 to 3\n",
    "            if 0 <= idx + offset < len(beats) and offset != 0:  # Check boundaries and exclude the cue itself\n",
    "                labels[idx + offset] = 0\n",
    "                cueno_indices.append(idx + offset)\n",
    "\n",
    "    # Diagnostic print statements\n",
    "    print(\"Cueyes Indices:\", cueyes_indices)\n",
    "    print(\"Cueno Indices:\", cueno_indices)\n",
    "\n",
    "    return labels, cueyes_indices, cueno_indices\n",
    "\n",
    "# Apply the function to all tracks\n",
    "for track_name, track in tracks.items():\n",
    "    track.labels, track.cueyes, track.cueno = label_cues(track.beats, track.cueyes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48d44cc8",
   "metadata": {},
   "source": [
    "total_cueyes = 0\n",
    "total_cueno = 0\n",
    "\n",
    "for track_name, track in tracks.items():\n",
    "    total_cueyes += len(track.cueyes)\n",
    "    total_cueno += len(track.cueno)\n",
    "\n",
    "print(f\"Total Cueyes: {total_cueyes}\")\n",
    "print(f\"Total Cueno: {total_cueno}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4210c93",
   "metadata": {},
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(audio, sr, timestamp, window_size=1.0):\n",
    "    \"\"\"\n",
    "    Extract multiple audio features for a given timestamp using librosa.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio: The audio data as a numpy array.\n",
    "    - sr: Sample rate.\n",
    "    - timestamp: The given timestamp (in seconds).\n",
    "    - window_size: The size of the window around the timestamp (in seconds, default is 1 second).\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary with feature names as keys and extracted values as values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the start and end samples for the window\n",
    "    center_sample = int(timestamp * sr)\n",
    "    start_sample = center_sample - int(sr * window_size / 2)\n",
    "    end_sample = center_sample + int(sr * window_size / 2)\n",
    "    \n",
    "    # Check for edge cases\n",
    "    if start_sample < 0:\n",
    "        start_sample = 0\n",
    "    if end_sample > len(audio):\n",
    "        end_sample = len(audio)\n",
    "\n",
    "    windowed_audio = audio[start_sample:end_sample]\n",
    "\n",
    "    # If windowed_audio is too small, return an empty dictionary\n",
    "    if len(windowed_audio) <= 1:\n",
    "        return {}\n",
    "\n",
    "    # Ensure even size for windowed_audio by padding with zero if necessary\n",
    "    if len(windowed_audio) % 2 != 0:\n",
    "        windowed_audio = np.append(windowed_audio, 0)\n",
    "\n",
    "    # Extract MFCC\n",
    "    mfcc_coeffs = librosa.feature.mfcc(y=windowed_audio, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # Extract Spectral Centroid\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=windowed_audio, sr=sr)\n",
    "\n",
    "    # Extract Spectral Contrast\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=windowed_audio, sr=sr)\n",
    "\n",
    "    # Extract Spectral Rolloff\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=windowed_audio, sr=sr)\n",
    "\n",
    "    # Extract Spectral Flux - this is the difference between consecutive spectral frames\n",
    "    spectrum = librosa.stft(windowed_audio)\n",
    "    flux = np.diff(np.abs(spectrum), axis=1)\n",
    "\n",
    "    # Calculate RMS for the beat\n",
    "    rms = librosa.feature.rms(y=windowed_audio)\n",
    "\n",
    "    features = {\n",
    "        \"MFCC\": mfcc_coeffs.mean(axis=1),  # Averaging over time frames\n",
    "        \"Spectral Centroid\": np.mean(spec_centroid),\n",
    "        \"Spectral Contrast\": np.mean(spec_contrast, axis=1),  # Averaging over frequency bands\n",
    "        \"Spectral Rolloff\": np.mean(spec_rolloff),\n",
    "        \"Spectral Flux\": np.mean(flux),\n",
    "        \"RMS\": np.mean(rms)\n",
    "    }\n",
    "\n",
    "    return features"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cef68dbe",
   "metadata": {},
   "source": [
    "def extract_features_for_labeled_beats(track):\n",
    "    all_features = []\n",
    "    for idx, beat_time in enumerate(track.beats):\n",
    "        # Check if the beat has a label of 1 (cueyes) or 0 (cueno)\n",
    "        if track.labels[idx] in [0, 1]:\n",
    "            features = extract_features(track.audio, track.sr, timestamp=beat_time)\n",
    "            features['label'] = 1 if track.labels[idx] == 1 else 0  # 'CUE' or 'NON-CUE' can be used instead of 1 or 0 if needed\n",
    "            all_features.append(features)\n",
    "    return all_features\n",
    "\n",
    "# Extract features for all tracks and check the lengths\n",
    "for track_name, track in tracks.items():\n",
    "    track.features = extract_features_for_labeled_beats(track)\n",
    "    print(f\"Track: {track_name}\")\n",
    "    print(f\"Expected Features (from cueyes + cueno): {len(track.cueyes) + len(track.cueno)}\")\n",
    "    print(f\"Actual Extracted Features: {len(track.features)}\")\n",
    "    print(\"------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d907afb",
   "metadata": {},
   "source": [
    "def convert_to_feature_vector(beat_features):\n",
    "    \"\"\"Convert beat features into a flattened feature vector.\"\"\"\n",
    "    # Flatten MFCC and Spectral Contrast\n",
    "    flat_features = list(beat_features['MFCC']) + \\\n",
    "                    list(beat_features['Spectral Contrast']) + \\\n",
    "                    [beat_features['Spectral Centroid']] + \\\n",
    "                    [beat_features['Spectral Rolloff']] + \\\n",
    "                    [beat_features['Spectral Flux']] + \\\n",
    "                    [beat_features['RMS']]\n",
    "    return flat_features\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for track_name, track in tracks.items():\n",
    "    for features in track.features:\n",
    "        # Extract features and labels\n",
    "        X.append(convert_to_feature_vector(features))\n",
    "        y.append(features['label'])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1550bcf",
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "\n",
    "# Reshaping the data to add an additional sequence dimension\n",
    "X_train_tensor = X_train_tensor.unsqueeze(1)  # shape becomes [batch_size, 1, input_dim]\n",
    "X_test_tensor = X_test_tensor.unsqueeze(1)    # shape becomes [batch_size, 1, input_dim]\n",
    "\n",
    "\n",
    "def init_hidden(batch_size):\n",
    "    return (torch.zeros(num_layers, batch_size, hidden_dim),\n",
    "            torch.zeros(num_layers, batch_size, hidden_dim))\n",
    "\n",
    "# Define the LSTM model without dropout\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # LSTM layer\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "    \n",
    "        # Get output for each time step in the sequence\n",
    "        out = self.fc(out).squeeze(1)  # We squeeze the sequence length dimension, which is 1 in this case\n",
    "    \n",
    "        return torch.sigmoid(out), hidden\n",
    "    \n",
    "hidden_dim = 10\n",
    "num_layers = 1\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 1\n",
    "    \n",
    "lstm_model = LSTMNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Xavier initialization\n",
    "for name, param in lstm_model.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        nn.init.constant_(param, 0.0)\n",
    "    elif 'weight' in name:\n",
    "        nn.init.xavier_normal_(param)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer with reduced learning rate\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.0005)  \n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "batch_size = 8  # As discussed\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train_tensor) - batch_size + 1, batch_size):  \n",
    "        \n",
    "        # Get mini-batch\n",
    "        inputs = X_train_tensor[i:i+batch_size]\n",
    "        labels = y_train_tensor[i:i+batch_size]\n",
    "\n",
    "        # Ensure the batch size is consistent for inputs and labels\n",
    "        actual_batch_size = min(batch_size, len(inputs), len(labels))\n",
    "        inputs = inputs[:actual_batch_size]\n",
    "        labels = labels[:actual_batch_size]\n",
    "\n",
    "        # Initialize hidden states\n",
    "        h0, c0 = init_hidden(actual_batch_size)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, _ = lstm_model(inputs, (h0, c0))\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "final_train_loss = train_losses[-1]  # Return the final training loss\n",
    "# final_train_loss\n",
    "\n",
    "# 4. Evaluation \n",
    "\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    h0, c0 = init_hidden(len(X_test_tensor))\n",
    "    outputs, _ = lstm_model(X_test_tensor, (h0, c0))\n",
    "    predictions = (outputs.squeeze() > 0.5).float()\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "y_test_flattened = y_test_tensor.numpy()\n",
    "\n",
    "accuracy = accuracy_score(y_test_flattened, predictions)\n",
    "precision = precision_score(y_test_flattened, predictions)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "235b5077",
   "metadata": {},
   "source": [
    "print(\"Results: \")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "28125ee6",
   "metadata": {},
   "source": [
    "### Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "id": "8dda71bd",
   "metadata": {},
   "source": [
    "# it was lstm model 1 but dont want to override it \n",
    "\n",
    "torch.save(lstm_model.state_dict(), \"lstm_model_demo.pth\")\n",
    "\n",
    "# same here for the scaler \n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "dump(scaler, 'scaler_demo.joblib')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df051d46",
   "metadata": {},
   "source": [
    "### Testing the Cue Point Generator Model on unseen data\n",
    "### 1. Alec Dienar - Perpetual Groove EP \n",
    "### 2. Impulsive Behaviour - unreleased tracks "
   ]
  },
  {
   "cell_type": "code",
   "id": "50dd5ff5",
   "metadata": {},
   "source": [
    "# Specify your paths\n",
    "path1 = \"AlecDienaar-PerpetualGroove\"\n",
    "path2 = \"ImpulsiveBehaviour-Unreleased\"\n",
    "\n",
    "paths = [path1, path2]\n",
    "all_tracks = {}\n",
    "counter = {}\n",
    "\n",
    "for path in paths:\n",
    "    wav_files = [os.path.join(path, file_name) for file_name in os.listdir(path) if file_name.endswith(('.wav', '.aif'))]\n",
    "\n",
    "    for wav_file in wav_files:\n",
    "        track_name = (os.path.basename(wav_file))\n",
    "        if track_name in counter:\n",
    "            counter[track_name] += 1\n",
    "            track_name = f\"{track_name}{counter[track_name]}\"\n",
    "        else:\n",
    "            counter[track_name] = 1\n",
    "        track = SimpleTrack(track_name, wav_file)\n",
    "        all_tracks[track_name] = track\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d36baf63",
   "metadata": {},
   "source": [
    "all_tracks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b736279",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Calling the preprocess function on each track in the all_tracks dictionary\n",
    "for track_name, track_obj in all_tracks.items():\n",
    "    print(f\"Preprocessing : {track_obj.name}\")\n",
    "    preprocess_simple_track(track_obj)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5e6f2bd",
   "metadata": {},
   "source": [
    "new_data_dict = all_tracks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67f39c0a",
   "metadata": {},
   "source": [
    "def extract_features_for_all_beats(track):\n",
    "    all_features = []\n",
    "    for beat_time in track.beats:\n",
    "        features = extract_features(track.audio, track.sr, timestamp=beat_time)\n",
    "        # If you still want to include the labels for some tracks (like the training tracks), you can do:\n",
    "        # features['label'] = 1 if track.labels[idx] == 1 else 0\n",
    "        all_features.append(features)\n",
    "    return all_features\n",
    "\n",
    "# Extract features for all tracks and check the lengths\n",
    "for track_name, track in new_data_dict.items():\n",
    "    track.features = extract_features_for_all_beats(track)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e31833c6",
   "metadata": {},
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def get_cue_point_timestamps(track, cue_point_indices):\n",
    "    return [track.beats[i] for i in cue_point_indices]\n",
    "\n",
    "def init_hidden(batch_size):\n",
    "    return (torch.zeros(num_layers, batch_size, hidden_dim),\n",
    "            torch.zeros(num_layers, batch_size, hidden_dim))\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out).squeeze(1)\n",
    "        return torch.sigmoid(out), hidden\n",
    "    \n",
    "# Parameters \n",
    "\n",
    "hidden_dim = 10\n",
    "num_layers = 1\n",
    "input_dim = 24 # length of the flat feature vector\n",
    "output_dim = 1\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "# Load the scaler and model\n",
    "scaler = load('scaler_demo.joblib')\n",
    "model = LSTMNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model.load_state_dict(torch.load(\"lstm_model_demo.pth\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "def categorize_cue_points(cue_points):\n",
    "    # Define the four arrays\n",
    "    arrays = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: [],\n",
    "        3: []\n",
    "    }\n",
    "    \n",
    "    for point in cue_points:\n",
    "        remainder = point % 4\n",
    "        arrays[remainder].append(point)\n",
    "    \n",
    "    # Find the array with the most cue points\n",
    "    max_key = max(arrays, key=lambda k: len(arrays[k]))\n",
    "    \n",
    "    return max_key\n",
    "    \n",
    "def detect_cue_points_for_track(track, num_cue_points=12, filter_by=None):\n",
    "    with torch.no_grad():\n",
    "        h0, c0 = init_hidden(1)  # Batch size of 1\n",
    "        outputs, _ = model(track.x_tensor, (h0, c0))\n",
    "        predicted_cues = outputs.squeeze().tolist()\n",
    "\n",
    "    # If filtering is applied\n",
    "    if filter_by is not None:\n",
    "        filtered_indices = [i for i in range(len(predicted_cues)) if i % 4 == filter_by]\n",
    "        filtered_cues = [predicted_cues[i] for i in filtered_indices]\n",
    "    else:\n",
    "        filtered_cues = predicted_cues\n",
    "        filtered_indices = list(range(len(predicted_cues)))\n",
    "\n",
    "    # Split the track into n intervals and get the top cue point in each interval\n",
    "    interval_length = len(predicted_cues) // num_cue_points  # We use the full track's length\n",
    "    cue_indices = []\n",
    "\n",
    "    for i in range(num_cue_points):\n",
    "        start = i * interval_length\n",
    "        end = (i + 1) * interval_length\n",
    "        \n",
    "        # Get the corresponding start and end indices for the filtered cues\n",
    "        filtered_start = bisect_left(filtered_indices, start)\n",
    "        filtered_end = bisect_left(filtered_indices, end)\n",
    "        \n",
    "        # Clip the end if it exceeds the filtered cues length\n",
    "        filtered_end = min(filtered_end, len(filtered_cues) - 1)\n",
    "        \n",
    "        interval_values = filtered_cues[filtered_start:filtered_end]\n",
    "        \n",
    "        # Skip if interval_values is empty\n",
    "        if not interval_values:\n",
    "            continue\n",
    "        \n",
    "        max_index_in_interval = np.argmax(interval_values)\n",
    "        original_index = filtered_indices[filtered_start + max_index_in_interval]\n",
    "        cue_indices.append(original_index)\n",
    "\n",
    "    track.cue_points_indeces = np.array(cue_indices)\n",
    "    track.cue_points = get_cue_point_timestamps(track, track.cue_points_indeces)\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "617f0f57",
   "metadata": {},
   "source": [
    "for track_name, track in new_data_dict.items():\n",
    "    track.prepare_features_for_prediction()\n",
    "    \n",
    "    # First run to determine the dominant array\n",
    "    detect_cue_points_for_track(track, num_cue_points=64)\n",
    "    dominant_key = categorize_cue_points(track.cue_points_indeces)\n",
    "    \n",
    "    # Second run with filtering\n",
    "    detect_cue_points_for_track(track, num_cue_points=12, filter_by=dominant_key)\n",
    "    \n",
    "    print(f\"Track: {track_name}\")\n",
    "    print(f\"Cue Points (indices): {track.cue_points_indeces}\")\n",
    "    print(f\"Cue Points: {track.cue_points}\")\n",
    "    print(\"------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff431646",
   "metadata": {},
   "source": [
    "def plot_waveform_with_hot_cues(audio, name, sr, hot_cues):\n",
    "    # Create a time array for the waveform\n",
    "    duration = len(audio) / sr\n",
    "    times = np.linspace(0, duration, len(audio))\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(times, audio, alpha=0.6)\n",
    "\n",
    "    # Plot the hot cues as vertical lines\n",
    "    for hot_cue in hot_cues:\n",
    "        plt.axvline(x=hot_cue, color='r')\n",
    "\n",
    "    # Get the first word of the name\n",
    "    first_word = name.split(' ')[0]\n",
    "\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title(f\"Waveform with Hot Cues: {first_word}\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6cff33e",
   "metadata": {},
   "source": [
    "new_data_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97bf6864",
   "metadata": {},
   "source": [
    "for track_name, track in new_data_dict.items():\n",
    "    plot_waveform_with_hot_cues(track.audio, track.name, track.sr, track.cue_points)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "607563a8",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
